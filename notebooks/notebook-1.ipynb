{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538ecc63",
   "metadata": {},
   "source": [
    "\n",
    "### Installing  prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3faca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If you already installed these in your virtual environment, you can skip this cell.\n",
    "%pip install -q \\\n",
    "  langchain>=0.2.0 \\\n",
    "  langchain-community \\\n",
    "  langchain-text-splitters \\\n",
    "  faiss-cpu>=1.7.4 \\\n",
    "  pandas>=2.0 \\\n",
    "  openpyxl>=3.1 \\\n",
    "  python-docx>=1.1 \\\n",
    "  pypdf>=4.2 \\\n",
    "  tqdm \\\n",
    "  python-dotenv \\\n",
    "  openai \\\n",
    "  langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d220785c",
   "metadata": {},
   "source": [
    "\n",
    "### Imports & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# *** OpenAI embeddings only ***\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Sanity check to verify OpenAI is being used\n",
    "vec = embeddings.embed_query(\"hello from openai\")\n",
    "print(\"OpenAI embedding vector length:\", len(vec))\n",
    "\n",
    "\n",
    "ROOT = Path.cwd().parent        \n",
    "DATA_DIR = ROOT / \"data\"        \n",
    "SAVE_DIR = DATA_DIR / \"cs-standards-vector-store\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR, SAVE_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bd2ed",
   "metadata": {},
   "source": [
    "\n",
    "### Load documents from `data/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mkdoc(text: str, meta: dict):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    return Document(page_content=text, metadata=meta)\n",
    "\n",
    "def load_documents(data_dir: Path) -> list[Document]:\n",
    "    docs = []\n",
    "    for f in sorted(data_dir.glob(\"**/*\")):\n",
    "        if not f.is_file():\n",
    "            continue\n",
    "        ext = f.suffix.lower()\n",
    "\n",
    "        if ext in [\".xlsx\", \".csv\"]:\n",
    "            df = pd.read_excel(f) if ext == \".xlsx\" else pd.read_csv(f)\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "            state_col   = cols.get(\"state\")\n",
    "            grade_col   = cols.get(\"grade\")\n",
    "            strand_col  = cols.get(\"strand\")\n",
    "            cluster_col = cols.get(\"cluster\") or cols.get(\"content cluster\") or cols.get(\"content_cluster\")\n",
    "            id_col      = cols.get(\"standard_id\") or cols.get(\"id\") or cols.get(\"code\")\n",
    "            text_col    = cols.get(\"standard_text\") or cols.get(\"standard\") or cols.get(\"description\") or cols.get(\"text\")\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                meta = {\n",
    "                    \"source\": str(f),\n",
    "                    \"state\":  row.get(state_col, \"\") if state_col else \"\",\n",
    "                    \"grade\":  row.get(grade_col, \"\") if grade_col else \"\",\n",
    "                    \"strand\": row.get(strand_col, \"\") if strand_col else \"\",\n",
    "                    \"cluster\":row.get(cluster_col, \"\") if cluster_col else \"\",\n",
    "                    \"standard_id\": row.get(id_col, \"\") if id_col else \"\",\n",
    "                }\n",
    "                body = str(row.get(text_col, \"\")) if text_col else \" | \".join(\n",
    "                    f\"{c}: {row.get(c, '')}\" for c in df.columns\n",
    "                )\n",
    "                text = (\n",
    "                    f\"State: {meta['state']} | Grade: {meta['grade']} | Strand: {meta['strand']} | \"\n",
    "                    f\"Cluster: {meta['cluster']} | StandardID: {meta['standard_id']}\\n{body}\"\n",
    "                )\n",
    "                d = _mkdoc(text, meta)\n",
    "                if d:\n",
    "                    docs.append(d)\n",
    "\n",
    "        elif ext == \".docx\":\n",
    "            from docx import Document as Docx\n",
    "            d = Docx(f)\n",
    "            text = \"\\n\".join(p.text for p in d.paragraphs)\n",
    "            d = _mkdoc(text, {\"source\": str(f)})\n",
    "            if d: docs.append(d)\n",
    "\n",
    "        elif ext == \".pdf\":\n",
    "            from pypdf import PdfReader\n",
    "            r = PdfReader(str(f))\n",
    "            text = \"\\n\".join((p.extract_text() or \"\") for p in r.pages)\n",
    "            d = _mkdoc(text, {\"source\": str(f)})\n",
    "            if d: docs.append(d)\n",
    "\n",
    "        elif ext in [\".txt\", \".md\"]:\n",
    "            text = f.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            d = _mkdoc(text, {\"source\": str(f)})\n",
    "            if d: docs.append(d)\n",
    "\n",
    "        # Ignore other extensions by default\n",
    "    return docs\n",
    "\n",
    "print(\"Scanning:\", DATA_DIR)\n",
    "raw_docs = load_documents(DATA_DIR)\n",
    "print(f\"Loaded {len(raw_docs)} raw docs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f3396",
   "metadata": {},
   "source": [
    "\n",
    "### Chunk documents\n",
    "Uses a `RecursiveCharacterTextSplitter` with `chunk_size=1000` and `chunk_overlap=200`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a82cb",
   "metadata": {},
   "source": [
    "\n",
    "### Build FAISS index (batch size = 16) and save\n",
    "\n",
    "We initialize with the first batch (up to 16 chunks), then add the remaining chunks in steps of 16.  \n",
    "Finally, we save the FAISS store to `vector_store_cs_standards/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(chunks) == 0:\n",
    "    raise ValueError(\"No chunks found. Make sure your data directory contains supported files.\")\n",
    "\n",
    "first_batch = chunks[:16] if len(chunks) >= 16 else chunks\n",
    "vector_store = FAISS.from_documents(first_batch, embedding=embeddings)\n",
    "\n",
    "BATCH = 16\n",
    "for i in tqdm(range(len(first_batch), len(chunks), BATCH), desc=\"Indexing\"):\n",
    "    vector_store.add_documents(chunks[i:i+BATCH])\n",
    "\n",
    "vector_store.save_local(str(SAVE_DIR))\n",
    "print(\"Saved FAISS index to:\", SAVE_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f161ab2",
   "metadata": {},
   "source": [
    "\n",
    "### Quick query test\n",
    "\n",
    "Load the saved index and run a similarity search.  \n",
    "> **Note:** Use the **same** embedding model class and pass `allow_dangerous_deserialization=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS as FAISSLoader\n",
    "\n",
    "vs = FAISSLoader.load_local(str(SAVE_DIR), embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "docs = vs.similarity_search(\"1st grade fractions standards in Texas\", k=5)\n",
    "for j, d in enumerate(docs, 1):\n",
    "    print(f\"\\n[{j}] {d.metadata}\")\n",
    "    print(d.page_content[:400], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
